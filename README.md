# Hadoop-and-Pyspark
Two projects work with Hadoop and PySpark on the University of Michigan flux clusters. The script is included as well as the shell script for submitting the script.  

## 1. Calculating sample statistics by group 
The job is done with mrjob.      

Given observations by group, calculate sample size, sample mean and sample variance by group. The MapReduce structure is used. 

## 2. Couting and Output triangles
The job is done with PySpark. 

Given nodes number and its connections, calculate all possible triangles. 
